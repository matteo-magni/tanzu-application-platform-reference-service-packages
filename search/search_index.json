{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TAP Reference Service Packages \u00b6 This repository is a collection of reference packages for Kubernetes, aimed at demonstrating how to consume public cloud services and expose the generated secrets to applications via service bindings. They are fully compatible with Services Toolkit for VMware Tanzu Application Platform . Either Carvel packages or Crosspplane configurations have been used, in order to provide examples for both methods. Important Such packages are intended to be for reference only and haven't been tested nor are they supported for production use.","title":"HOME"},{"location":"#tap-reference-service-packages","text":"This repository is a collection of reference packages for Kubernetes, aimed at demonstrating how to consume public cloud services and expose the generated secrets to applications via service bindings. They are fully compatible with Services Toolkit for VMware Tanzu Application Platform . Either Carvel packages or Crosspplane configurations have been used, in order to provide examples for both methods. Important Such packages are intended to be for reference only and haven't been tested nor are they supported for production use.","title":"TAP Reference Service Packages"},{"location":"crossplane/","text":"Install Crossplane via Upbound CLI \u00b6 Download the up cli curl -sL \"https://cli.upbound.io\" | sh sudo mv up /usr/local/bin/ Check the installed version: up --version Switch to the proper Kubernetes context and run the following command in order to install Upbound Universal Crossplane (UXP): up uxp install Verify all UXP pods are Running with kubectl get pods -n upbound-system. This may take up to five minutes depending on your Kubernetes cluster. $ k get pods -n upbound-system NAME READY STATUS RESTARTS AGE crossplane-65444df64-7wcb2 1 /1 Running 0 92s crossplane-rbac-manager-69498f955b-2npkl 1 /1 Running 0 92s upbound-bootstrapper-5c9864b546-lngkw 1 /1 Running 0 92s xgql-6485cf5748-src2w 1 /1 Running 3 ( 70s ago ) 92s Note RESTARTS for the xgql pod are normal during initial installation.","title":"Install Crossplane"},{"location":"crossplane/#install-crossplane-via-upbound-cli","text":"Download the up cli curl -sL \"https://cli.upbound.io\" | sh sudo mv up /usr/local/bin/ Check the installed version: up --version Switch to the proper Kubernetes context and run the following command in order to install Upbound Universal Crossplane (UXP): up uxp install Verify all UXP pods are Running with kubectl get pods -n upbound-system. This may take up to five minutes depending on your Kubernetes cluster. $ k get pods -n upbound-system NAME READY STATUS RESTARTS AGE crossplane-65444df64-7wcb2 1 /1 Running 0 92s crossplane-rbac-manager-69498f955b-2npkl 1 /1 Running 0 92s upbound-bootstrapper-5c9864b546-lngkw 1 /1 Running 0 92s xgql-6485cf5748-src2w 1 /1 Running 3 ( 70s ago ) 92s Note RESTARTS for the xgql pod are normal during initial installation.","title":"Install Crossplane via Upbound CLI"},{"location":"crossplane/providers/aws/","text":"Upbound Universal Crossplane (UXP) AWS provider is a provider for Amazon Web Services developed and supported by Upbound. It can be deployed on top of a Kubernetes cluster with Crossplane, by using either the Upbound CLI (see here for details about installation) or a YAML manifest. Installation \u00b6 You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-aws and store the desired release into the PROVIDER_AWS_RELEASE variable. Upbound CLI YAML manifest Do make sure you have installed the up CLI as described here and execute up controlplane provider install xpkg.upbound.io/upbound/provider-aws: ${ PROVIDER_AWS_RELEASE } kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-aws spec: package: xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE} EOF It is now necessary to configure the provider's authentication to the AWS API endpoints. The authentication method can vary based on your company's policies: for example, you might be allowed to use long-term credentials such as access key and secret access key pairs, however, if your Kubernetes platform is AWS EKS, it's much more secure to use IAM roles for service accounts (IRSA) . Please make sure you do create the OIDC provider as described in the EKS set-up guide before reading on. The following paragraphs explain how to configure IRSA for the Crossplane AWS provider. Create IAM role and policy \u00b6 You must create a proper role for the provider to assume, for granting the necessary and sufficient permissions to manage the AWS infrastructure. The least-privilege principle applies, therefore it's important to understand the actual needs and create the permission policy accordingly. For example, the following snippet creates a policy that allows the role it's attached to to execute actions only on the S3 service. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) CROSSPLANE_ROLE = Crossplane-for- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:upbound-system:upbound-provider-aws-*\" } } }] } EOF # create the role with the proper trust policy aws iam create-role --role-name ${ CROSSPLANE_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # create the permission policy document cat > ${ ROLE_PERMISSION_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"s3:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } EOF # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ CROSSPLANE_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ CROSSPLANE_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Create Kubernetes resources \u00b6 Create a ProviderConfig resource to specify IRSA as authentication method and the role ARN that must be assumed: kubectl apply -f - <<EOF apiVersion: aws.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: IRSA webIdentity: roleARN: arn:aws:iam::${ACCOUNT_ID}:role/${CROSSPLANE_ROLE} EOF Now you can test the effectiveness of the configuration by creating a simple S3 bucket: BUCKET_NAME = $( kubectl create -o yaml -f - <<EOF | yq '.metadata.name' apiVersion: s3.aws.upbound.io/v1beta1 kind: Bucket metadata: generateName: crossplane-test-bucket- spec: forProvider: region: ${AWS_REGION} EOF ) and verifying its status $ kubectl get buckets.s3.aws.upbound.io ${ BUCKET_NAME } NAME READY SYNCED EXTERNAL-NAME AGE crossplane-test-bucket-cxr9g True True crossplane-test-bucket-cxr9g 80s As the bucket is marked as synced, it's worth checking the status of the AWS resource: $ aws s3api list-buckets | jq '.Buckets[]|select(.Name == \"' ${ BUCKET_NAME } '\")' { \"Name\" : \"crossplane-test-bucket-cxr9g\" , \"CreationDate\" : \"2022-11-05T00:36:49+00:00\" } This proves that the provider is configured correctly, you can now delete the test bucket: kubectl delete buckets.s3.aws.upbound.io ${ BUCKET_NAME }","title":"UXP AWS provider"},{"location":"crossplane/providers/aws/#installation","text":"You can check available releases on project's GitHub repository or using gh like gh release list --repo upbound/provider-aws and store the desired release into the PROVIDER_AWS_RELEASE variable. Upbound CLI YAML manifest Do make sure you have installed the up CLI as described here and execute up controlplane provider install xpkg.upbound.io/upbound/provider-aws: ${ PROVIDER_AWS_RELEASE } kubectl apply -f - <<EOF apiVersion: pkg.crossplane.io/v1 kind: Provider metadata: name: provider-aws spec: package: xpkg.upbound.io/upbound/provider-aws:${PROVIDER_AWS_RELEASE} EOF It is now necessary to configure the provider's authentication to the AWS API endpoints. The authentication method can vary based on your company's policies: for example, you might be allowed to use long-term credentials such as access key and secret access key pairs, however, if your Kubernetes platform is AWS EKS, it's much more secure to use IAM roles for service accounts (IRSA) . Please make sure you do create the OIDC provider as described in the EKS set-up guide before reading on. The following paragraphs explain how to configure IRSA for the Crossplane AWS provider.","title":"Installation"},{"location":"crossplane/providers/aws/#create-iam-role-and-policy","text":"You must create a proper role for the provider to assume, for granting the necessary and sufficient permissions to manage the AWS infrastructure. The least-privilege principle applies, therefore it's important to understand the actual needs and create the permission policy accordingly. For example, the following snippet creates a policy that allows the role it's attached to to execute actions only on the S3 service. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) CROSSPLANE_ROLE = Crossplane-for- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:upbound-system:upbound-provider-aws-*\" } } }] } EOF # create the role with the proper trust policy aws iam create-role --role-name ${ CROSSPLANE_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # create the permission policy document cat > ${ ROLE_PERMISSION_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"s3:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } EOF # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ CROSSPLANE_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ CROSSPLANE_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY }","title":"Create IAM role and policy"},{"location":"crossplane/providers/aws/#create-kubernetes-resources","text":"Create a ProviderConfig resource to specify IRSA as authentication method and the role ARN that must be assumed: kubectl apply -f - <<EOF apiVersion: aws.upbound.io/v1beta1 kind: ProviderConfig metadata: name: default spec: credentials: source: IRSA webIdentity: roleARN: arn:aws:iam::${ACCOUNT_ID}:role/${CROSSPLANE_ROLE} EOF Now you can test the effectiveness of the configuration by creating a simple S3 bucket: BUCKET_NAME = $( kubectl create -o yaml -f - <<EOF | yq '.metadata.name' apiVersion: s3.aws.upbound.io/v1beta1 kind: Bucket metadata: generateName: crossplane-test-bucket- spec: forProvider: region: ${AWS_REGION} EOF ) and verifying its status $ kubectl get buckets.s3.aws.upbound.io ${ BUCKET_NAME } NAME READY SYNCED EXTERNAL-NAME AGE crossplane-test-bucket-cxr9g True True crossplane-test-bucket-cxr9g 80s As the bucket is marked as synced, it's worth checking the status of the AWS resource: $ aws s3api list-buckets | jq '.Buckets[]|select(.Name == \"' ${ BUCKET_NAME } '\")' { \"Name\" : \"crossplane-test-bucket-cxr9g\" , \"CreationDate\" : \"2022-11-05T00:36:49+00:00\" } This proves that the provider is configured correctly, you can now delete the test bucket: kubectl delete buckets.s3.aws.upbound.io ${ BUCKET_NAME }","title":"Create Kubernetes resources"},{"location":"usecases/aws/packages/elasticache/ack/","text":"This guide describes using the Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache instances. Note This usecase is not currently compatible with TAP air-gapped installations. Prerequisites \u00b6 You need to meet a number of prerequisites before being able to effectively follow this guide. Create service instances that are compatible with Tanzu Application Platform \u00b6 The installation of the AWS Elasticache Controller for Kubernetes results in the availability of new Kubernetes APIs for interacting with Elasticache resources from within the TAP cluster. kubectl api-resources --api-group elasticache.services.k8s.aws NAME SHORTNAMES APIVERSION NAMESPACED KIND cacheparametergroups elasticache.services.k8s.aws/v1alpha1 true CacheParameterGroup cachesubnetgroups elasticache.services.k8s.aws/v1alpha1 true CacheSubnetGroup replicationgroups elasticache.services.k8s.aws/v1alpha1 true ReplicationGroup snapshots elasticache.services.k8s.aws/v1alpha1 true Snapshot usergroups elasticache.services.k8s.aws/v1alpha1 true UserGroup users elasticache.services.k8s.aws/v1alpha1 true User To create an AWS Elasticache service instance for consumption by Tanzu Application Platform, you can use a ready-made, reference Carvel Package. The Service Operator typically performs this step. Follow the steps in Creating an AWS Elasticache service instance using a Carvel Package . Alternatively, if you are interested in authoring your own Reference Package and want to learn about the underlying APIs and how they come together to produce a useable service instance for the Tanzu Application Platform, you can achieve the same outcome using the more advanced Creating an AWS Elasticache service instance manually . Once you have completed either of these steps and have a running AWS Elasticache service instance, please return here to continue with the rest of the use case. Create a service instance class for AWS Elasticache \u00b6 Now that you know how to create AWS Elasticache instances, it's time to learn how to make those instances discoverable to Application Operators. Again, this step is typically performed by the service operator persona. You can use Services Toolkit's ClusterInstanceClass API to create a service instance class to represent Elasticache service instances within the cluster. The existence of such classes makes these logical service instances discoverable to application operators, thus allowing them to create [Resource Claims][resource-claims] for such instances and to then bind them to application workloads. Create the following Kubernetes resource on your AKS cluster: clusterinstanceclass.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ClusterInstanceClass metadata : name : aws-elasticache spec : description : short : AWS Elasticache instances pool : kind : Secret labelSelector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache kubectl apply -f clusterinstanceclass.yaml In this particular example, the class represents claimable instances of Postgresql by a Secret object with the label services.apps.tanzu.vmware.com/class set to aws-elasticache . In addition, you need to grant sufficient RBAC permissions to Services Toolkit to be able to read the secrets specified by the class. Create the following RBAC on your AKS cluster: clusterrole.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : stk-secret-reader labels : servicebinding.io/controller : \"true\" rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch kubectl apply -f clusterrole.yaml If you want to claim resources across namespace boundaries, you will have to create a corresponding ResourceClaimPolicy . For example, if the provisioned AWS Elasticache instance named redis exists in namespace service-instances and you want to allow App Operators to claim them for workloads residing in the default namespace, you would have to create the following ResourceClaimPolicy : resourceclaimpolicy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ResourceClaimPolicy metadata : name : default-can-claim-aws-elasticache namespace : service-instances spec : subject : kind : Secret group : \"\" selector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache consumingNamespaces : [ \"default\" ] kubectl apply -f resourceclaimpolicy.yaml Discover, Claim, and Bind to an AWS Elasticache for Redis instance \u00b6 The act of creating the ClusterInstanceClass and the corresponding RBAC essentially advertises to application operators that AWS Elasticache for Redis is available to use with their application workloads on Tanzu Application Platform. In this section, you learn how to discover, claim, and bind to the AWS Elasticache service instance previously created. Discovery and claiming service instances is typically the responsibility of the application operator persona. Binding is typically a step for Application Developers. To discover what service instances are available to them, application operators can run: $ tanzu services classes list NAME DESCRIPTION aws-elasticache AWS Elasticache instances You can see information about the ClusterInstanceClass created in the previous step. Each ClusterInstanceClass created will be added to the list of classes returned here. The next step is to \"claim\" an instance of the desired class, but to do that, the application operators must first discover the list of currently claimable instances for the class. The capacity to claim instances is affected by many variables (including namespace boundaries, claim policies, and the exclusivity of claims) and so Services Toolkit provides a CLI command to help inform application operators of the instances that can result in successful claims. This command is the tanzu service claimable list command. $ tanzu services claimable list --class aws-elasticache -n default NAME NAMESPACE KIND APIVERSION redis-reader-creds-bindable service-instances Secret v1 redis-writer-creds-bindable service-instances Secret v1 Create a claim for the newly created secret by running: tanzu services claim create redis-writer-claim \\ --namespace default \\ --resource-namespace service-instances \\ --resource-name redis-writer-creds-bindable \\ --resource-kind Secret \\ --resource-api-version v1 Obtain the claim reference of the claim by running: $ tanzu services claim list -o wide NAME READY REASON CLAIM REF redis-writer-claim True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim Test Claim With TAP Workload \u00b6 Create an application workload that consumes the claimed AWS Elasticache by running: Example: tanzu apps workload create my-workload \\ --git-repo <a-git-repo> \\ --git-tag <a-tag-to-checkout> \\ --type web \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim --service-ref is set to the claim reference obtained previously. Your application workload starts and gets automatically the credentials to the AWS Elasticache instance via service bindings. Delete an AWS Elasticache service instance resources \u00b6 To delete the AWS Elasticache service instance, you can run the appropriate cleanup commands for how you created the service. Delete an AWS Elasticache instance via Carvel Package \u00b6 tanzu package installed delete redis-instance Delete an AWS Elasticache instance via Kubectl \u00b6 Delete the AWS Elasticache instance by running: kubectl delete -n service-instances replicationgroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances usergroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-default kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-reader kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-writer kubectl delete -n service-instances cachesubnetgroups.elasticache.services.k8s.aws ack-elasticache kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-reader-creds-bindable kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-writer-creds-bindable kubectl delete -n service-instances password.secretgen.carvel.dev redis-reader-creds kubectl delete -n service-instances password.secretgen.carvel.dev redis-writer-creds kubectl delete -n service-instances serviceaccounts redis-elasticache-reader kubectl delete -n service-instances role redis-elasticache-reader kubectl delete -n service-instances rolebinding redis-elasticache-reader","title":"Consuming AWS Elasticache with ACK"},{"location":"usecases/aws/packages/elasticache/ack/#prerequisites","text":"You need to meet a number of prerequisites before being able to effectively follow this guide.","title":"Prerequisites"},{"location":"usecases/aws/packages/elasticache/ack/#create-service-instances-that-are-compatible-with-tanzu-application-platform","text":"The installation of the AWS Elasticache Controller for Kubernetes results in the availability of new Kubernetes APIs for interacting with Elasticache resources from within the TAP cluster. kubectl api-resources --api-group elasticache.services.k8s.aws NAME SHORTNAMES APIVERSION NAMESPACED KIND cacheparametergroups elasticache.services.k8s.aws/v1alpha1 true CacheParameterGroup cachesubnetgroups elasticache.services.k8s.aws/v1alpha1 true CacheSubnetGroup replicationgroups elasticache.services.k8s.aws/v1alpha1 true ReplicationGroup snapshots elasticache.services.k8s.aws/v1alpha1 true Snapshot usergroups elasticache.services.k8s.aws/v1alpha1 true UserGroup users elasticache.services.k8s.aws/v1alpha1 true User To create an AWS Elasticache service instance for consumption by Tanzu Application Platform, you can use a ready-made, reference Carvel Package. The Service Operator typically performs this step. Follow the steps in Creating an AWS Elasticache service instance using a Carvel Package . Alternatively, if you are interested in authoring your own Reference Package and want to learn about the underlying APIs and how they come together to produce a useable service instance for the Tanzu Application Platform, you can achieve the same outcome using the more advanced Creating an AWS Elasticache service instance manually . Once you have completed either of these steps and have a running AWS Elasticache service instance, please return here to continue with the rest of the use case.","title":"Create service instances that are compatible with Tanzu Application Platform"},{"location":"usecases/aws/packages/elasticache/ack/#create-a-service-instance-class-for-aws-elasticache","text":"Now that you know how to create AWS Elasticache instances, it's time to learn how to make those instances discoverable to Application Operators. Again, this step is typically performed by the service operator persona. You can use Services Toolkit's ClusterInstanceClass API to create a service instance class to represent Elasticache service instances within the cluster. The existence of such classes makes these logical service instances discoverable to application operators, thus allowing them to create [Resource Claims][resource-claims] for such instances and to then bind them to application workloads. Create the following Kubernetes resource on your AKS cluster: clusterinstanceclass.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ClusterInstanceClass metadata : name : aws-elasticache spec : description : short : AWS Elasticache instances pool : kind : Secret labelSelector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache kubectl apply -f clusterinstanceclass.yaml In this particular example, the class represents claimable instances of Postgresql by a Secret object with the label services.apps.tanzu.vmware.com/class set to aws-elasticache . In addition, you need to grant sufficient RBAC permissions to Services Toolkit to be able to read the secrets specified by the class. Create the following RBAC on your AKS cluster: clusterrole.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : stk-secret-reader labels : servicebinding.io/controller : \"true\" rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch kubectl apply -f clusterrole.yaml If you want to claim resources across namespace boundaries, you will have to create a corresponding ResourceClaimPolicy . For example, if the provisioned AWS Elasticache instance named redis exists in namespace service-instances and you want to allow App Operators to claim them for workloads residing in the default namespace, you would have to create the following ResourceClaimPolicy : resourceclaimpolicy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- apiVersion : services.apps.tanzu.vmware.com/v1alpha1 kind : ResourceClaimPolicy metadata : name : default-can-claim-aws-elasticache namespace : service-instances spec : subject : kind : Secret group : \"\" selector : matchLabels : services.apps.tanzu.vmware.com/class : aws-elasticache consumingNamespaces : [ \"default\" ] kubectl apply -f resourceclaimpolicy.yaml","title":"Create a service instance class for AWS Elasticache"},{"location":"usecases/aws/packages/elasticache/ack/#discover-claim-and-bind-to-an-aws-elasticache-for-redis-instance","text":"The act of creating the ClusterInstanceClass and the corresponding RBAC essentially advertises to application operators that AWS Elasticache for Redis is available to use with their application workloads on Tanzu Application Platform. In this section, you learn how to discover, claim, and bind to the AWS Elasticache service instance previously created. Discovery and claiming service instances is typically the responsibility of the application operator persona. Binding is typically a step for Application Developers. To discover what service instances are available to them, application operators can run: $ tanzu services classes list NAME DESCRIPTION aws-elasticache AWS Elasticache instances You can see information about the ClusterInstanceClass created in the previous step. Each ClusterInstanceClass created will be added to the list of classes returned here. The next step is to \"claim\" an instance of the desired class, but to do that, the application operators must first discover the list of currently claimable instances for the class. The capacity to claim instances is affected by many variables (including namespace boundaries, claim policies, and the exclusivity of claims) and so Services Toolkit provides a CLI command to help inform application operators of the instances that can result in successful claims. This command is the tanzu service claimable list command. $ tanzu services claimable list --class aws-elasticache -n default NAME NAMESPACE KIND APIVERSION redis-reader-creds-bindable service-instances Secret v1 redis-writer-creds-bindable service-instances Secret v1 Create a claim for the newly created secret by running: tanzu services claim create redis-writer-claim \\ --namespace default \\ --resource-namespace service-instances \\ --resource-name redis-writer-creds-bindable \\ --resource-kind Secret \\ --resource-api-version v1 Obtain the claim reference of the claim by running: $ tanzu services claim list -o wide NAME READY REASON CLAIM REF redis-writer-claim True Ready services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim","title":"Discover, Claim, and Bind to an AWS Elasticache for Redis instance"},{"location":"usecases/aws/packages/elasticache/ack/#test-claim-with-tap-workload","text":"Create an application workload that consumes the claimed AWS Elasticache by running: Example: tanzu apps workload create my-workload \\ --git-repo <a-git-repo> \\ --git-tag <a-tag-to-checkout> \\ --type web \\ --annotation autoscaling.knative.dev/minScale = 1 \\ --service-ref db = services.apps.tanzu.vmware.com/v1alpha1:ResourceClaim:redis-writer-claim --service-ref is set to the claim reference obtained previously. Your application workload starts and gets automatically the credentials to the AWS Elasticache instance via service bindings.","title":"Test Claim With TAP Workload"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-service-instance-resources","text":"To delete the AWS Elasticache service instance, you can run the appropriate cleanup commands for how you created the service.","title":"Delete an AWS Elasticache service instance resources"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-carvel-package","text":"tanzu package installed delete redis-instance","title":"Delete an AWS Elasticache instance via Carvel Package"},{"location":"usecases/aws/packages/elasticache/ack/#delete-an-aws-elasticache-instance-via-kubectl","text":"Delete the AWS Elasticache instance by running: kubectl delete -n service-instances replicationgroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances usergroup.elasticache.services.k8s.aws redis kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-default kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-reader kubectl delete -n service-instances user.elasticache.services.k8s.aws redis-writer kubectl delete -n service-instances cachesubnetgroups.elasticache.services.k8s.aws ack-elasticache kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-reader-creds-bindable kubectl delete -n service-instances secrettemplate.secretgen.carvel.dev redis-writer-creds-bindable kubectl delete -n service-instances password.secretgen.carvel.dev redis-reader-creds kubectl delete -n service-instances password.secretgen.carvel.dev redis-writer-creds kubectl delete -n service-instances serviceaccounts redis-elasticache-reader kubectl delete -n service-instances role redis-elasticache-reader kubectl delete -n service-instances rolebinding redis-elasticache-reader","title":"Delete an AWS Elasticache instance via Kubectl"},{"location":"usecases/aws/packages/elasticache/ack/manual/","text":"This topic describes how to use Services Toolkit to allow Tanzu Application Platform workloads to consume AWS Elasticache for Redis. This particular topic makes use of AWS Controller for Kubernetes (ACK) to manage AWS Elasticache resources. Following this guide, you will be creating all the resources in the service-instances namespace. It's important to make sure it exists before reading on. Create a cache subnet group \u00b6 The Elasticache instances must be created in a Cache Subnet Group, that can be created in a number of ways, here you will leverage ACK to create it. First of all, you have to get the IDs of the subnets you want to build the Cache Subnet Group for. Example You could have a list of all the subnets in a given VPC and then choose amongst them. VPC_NAME = \"my-vpc-name\" VPC_ID = $( aws ec2 describe-vpcs --filter \"Name=tag:Name,Values= ${ VPC_NAME } \" --query \"Vpcs[0].VpcId\" --output text ) SUBNET_IDS = $( aws ec2 describe-subnets --filters \"Name=vpc-id,Values= ${ VPC_ID } \" --query \"Subnets[].SubnetId\" ) This is just an example. Make sure you do choose your subnets carefully. When you have a list of subnetIDs stored in the SUBNET_IDS shell variable, you can create your CacheSubnetGroup . Create the following ytt template cache-subnet-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : CacheSubnetGroup metadata : name : ack-elasticache namespace : service-instances spec : cacheSubnetGroupDescription : A subnet group for Elasticache cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName subnetIDs : #@ data.values.subnetIDs Now you can use ytt to add the proper values and pipe it to kubectl apply CACHE_SUBNET_GROUP_NAME = \"ack-elasticache\" ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v subnetIDs = \" ${ SUBNET_IDS } \" \\ -f cache-subnet-group.ytt.yaml \\ | kubectl apply -f - Create the users to log into Elasticache \u00b6 You need at least one usergroup with at least one member user to associate to the Elasticache instance. Every group must have one user with name default and up to 100 total users (more on Elasticache quotas ). However, there can be only one user with id default per Elasticache instance , which is automatically made available by AWS, but there can be more users with name default and different id. This is useful to know in order to create proper default users for each group. For the sake of this example, you will create just one user group and a default user in it with all the permissions. In order to generate a random password, you will make use of Secretgen Controller , which is provided out of the box by TAP, thus if you went through the prerequisites it should have already been installed. The following snippet declares the default user along with its auto-generated password and the usergroup: elasticache-user.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : secretgen.k14s.io/v1alpha1 kind : Password metadata : name : ack-elasticache-default-creds namespace : service-instances spec : length : 128 secretTemplate : type : Opaque stringData : password : $(value) --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : User metadata : name : ack-elasticache-default namespace : service-instances spec : accessString : on ~* +@all engine : redis passwords : - name : ack-elasticache-default-creds key : password namespace : service-instances userID : ack-elasticache-default userName : default --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : UserGroup metadata : name : ack-elasticache namespace : service-instances spec : engine : redis userGroupID : ack-elasticache userIDs : - ack-elasticache-default Store it into the elasticache-user.yaml file and apply it: kubectl apply -f elasticache-user.yaml Create the ReplicationGroup \u00b6 Before going ahead and create the ReplicationGroup resource, which maps to the actual instance that can be consumed, you need to create a proper security group for filtering the incoming and outgoing traffic. Assuming that you want to be able to connect to Elasticache from the EKS instance created previously , you can use the EKS security group as source for the new security group which will actually filter the Elasticache traffic. # AWS region you're operating in export AWS_REGION = \"eu-west-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" EKS_SECURITY_GROUP_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text ) # VPC_ID has been defined above in \"Create a cache subnet group\" ELASTICACHE_SECURITY_GROUP_ID = $( aws ec2 create-security-group --group-name \"Elasticache\" --description \"Elasticache security group\" --vpc-id ${ VPC_ID } --output text --query GroupId ) REDIS_PORT = 6379 aws ec2 authorize-security-group-ingress --group-id ${ ELASTICACHE_SECURITY_GROUP_ID } --source-group ${ EKS_SECURITY_GROUP_ID } --protocol tcp --port ${ REDIS_PORT } Now you can define the ReplicationGroup using the following ytt template replication-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup metadata : name : ack-elasticache namespace : service-instances spec : description : A redis service instance engine : redis replicationGroupID : ack-elasticache cacheNodeType : cache.t2.micro cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName securityGroupIDs : - #@ data.values.securityGroupID userGroupIDs : - ack-elasticache and apply it ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v securityGroupID = \" ${ ELASTICACHE_SECURITY_GROUP_ID } \" \\ -f replication-group.ytt.yaml \\ | kubectl apply -f - It will take 5 to 10 minutes to create. You can wait for the resource to be ready running the command kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache or you can take a closer look at the new resource kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws ack-elasticache -o yaml particularly at the status field, which eventually will display something like status : ... conditions : - status : \"True\" type : ACK.ResourceSynced ... status : available The status object also contains the details of the provisioned AWS resource, along with the nodegroups and their endpoints. Create a Binding Specification Compatible Secret \u00b6 As mentioned in Creating service instances that are compatible with Tanzu Application Platform , in order for Tanzu Application Platform workloads to be able to claim and bind to services such as AWS Elasticache, a resource compatible with Service Binding Specification must exist in the cluster. This can take the form of either a ProvisionedService , as defined by the specification, or a Kubernetes Secret with some known keys, also as defined in the specification. In this guide, you create a Kubernetes secret in the necessary format using the secretgen-controller tooling. You do so by using the SecretTemplate API to extract values from the ACK resources and populate a new spec-compatible secret with the values. Create a ServiceAccount for Secret Templating \u00b6 As part of using the SecretTemplate API, a Kubernetes ServiceAccount must be provided. The ServiceAccount is used for reading the ReplicationGroup resource and the Secret created from the Password resource above. Create the following Kubernetes resources on your EKS cluster: rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : ack-elasticache-reader namespace : service-instances --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : ack-elasticache-reader namespace : service-instances rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch resourceNames : - ack-elasticache-default-creds - apiGroups : - elasticache.services.k8s.aws resources : - replicationgroups verbs : - get - list - watch resourceNames : - ack-elasticache --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-elasticache-reader namespace : service-instances roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : ack-elasticache-reader subjects : - kind : ServiceAccount name : ack-elasticache-reader namespace : service-instances kubectl apply -f rbac.yaml Create a SecretTemplate \u00b6 In combination with the ServiceAccount just created, a SecretTemplate can be used to declaratively create a secret that is compatible with the service binding specification. For more information on this API see the Secret Template Documentation . Create the following Kubernetes resource on your EKS cluster: secrettemplate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : secretgen.carvel.dev/v1alpha1 kind : SecretTemplate metadata : name : ack-elasticache-default-creds-bindable namespace : service-instances spec : serviceAccountName : ack-elasticache-reader inputResources : - name : replicationGroup ref : apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup name : ack-elasticache - name : creds ref : apiVersion : v1 kind : Secret name : ack-elasticache-default-creds template : metadata : labels : services.apps.tanzu.vmware.com/class : aws-elasticache type : servicebinding.io/redis stringData : type : redis username : $(.creds.data.username) ssl : \"true\" host : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.address) port : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.port) data : password : $(.creds.data.password) kubectl apply -f secrettemplate.yaml Verify the Service Instance \u00b6 Wait until the ReplicationGroup instance is ready as described before . Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True secrettemplates.secretgen.carvel.dev ack-elasticache-default-creds-bindable kubectl -n service-instances get secret ack-elasticache-default-creds-bindable","title":"Creating AWS Elasticache Instances manually using kubectl (experimental)"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-cache-subnet-group","text":"The Elasticache instances must be created in a Cache Subnet Group, that can be created in a number of ways, here you will leverage ACK to create it. First of all, you have to get the IDs of the subnets you want to build the Cache Subnet Group for. Example You could have a list of all the subnets in a given VPC and then choose amongst them. VPC_NAME = \"my-vpc-name\" VPC_ID = $( aws ec2 describe-vpcs --filter \"Name=tag:Name,Values= ${ VPC_NAME } \" --query \"Vpcs[0].VpcId\" --output text ) SUBNET_IDS = $( aws ec2 describe-subnets --filters \"Name=vpc-id,Values= ${ VPC_ID } \" --query \"Subnets[].SubnetId\" ) This is just an example. Make sure you do choose your subnets carefully. When you have a list of subnetIDs stored in the SUBNET_IDS shell variable, you can create your CacheSubnetGroup . Create the following ytt template cache-subnet-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : CacheSubnetGroup metadata : name : ack-elasticache namespace : service-instances spec : cacheSubnetGroupDescription : A subnet group for Elasticache cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName subnetIDs : #@ data.values.subnetIDs Now you can use ytt to add the proper values and pipe it to kubectl apply CACHE_SUBNET_GROUP_NAME = \"ack-elasticache\" ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v subnetIDs = \" ${ SUBNET_IDS } \" \\ -f cache-subnet-group.ytt.yaml \\ | kubectl apply -f -","title":"Create a cache subnet group"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-users-to-log-into-elasticache","text":"You need at least one usergroup with at least one member user to associate to the Elasticache instance. Every group must have one user with name default and up to 100 total users (more on Elasticache quotas ). However, there can be only one user with id default per Elasticache instance , which is automatically made available by AWS, but there can be more users with name default and different id. This is useful to know in order to create proper default users for each group. For the sake of this example, you will create just one user group and a default user in it with all the permissions. In order to generate a random password, you will make use of Secretgen Controller , which is provided out of the box by TAP, thus if you went through the prerequisites it should have already been installed. The following snippet declares the default user along with its auto-generated password and the usergroup: elasticache-user.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : secretgen.k14s.io/v1alpha1 kind : Password metadata : name : ack-elasticache-default-creds namespace : service-instances spec : length : 128 secretTemplate : type : Opaque stringData : password : $(value) --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : User metadata : name : ack-elasticache-default namespace : service-instances spec : accessString : on ~* +@all engine : redis passwords : - name : ack-elasticache-default-creds key : password namespace : service-instances userID : ack-elasticache-default userName : default --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : UserGroup metadata : name : ack-elasticache namespace : service-instances spec : engine : redis userGroupID : ack-elasticache userIDs : - ack-elasticache-default Store it into the elasticache-user.yaml file and apply it: kubectl apply -f elasticache-user.yaml","title":"Create the users to log into Elasticache"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-the-replicationgroup","text":"Before going ahead and create the ReplicationGroup resource, which maps to the actual instance that can be consumed, you need to create a proper security group for filtering the incoming and outgoing traffic. Assuming that you want to be able to connect to Elasticache from the EKS instance created previously , you can use the EKS security group as source for the new security group which will actually filter the Elasticache traffic. # AWS region you're operating in export AWS_REGION = \"eu-west-1\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" EKS_SECURITY_GROUP_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --query 'cluster.resourcesVpcConfig.clusterSecurityGroupId' --output text ) # VPC_ID has been defined above in \"Create a cache subnet group\" ELASTICACHE_SECURITY_GROUP_ID = $( aws ec2 create-security-group --group-name \"Elasticache\" --description \"Elasticache security group\" --vpc-id ${ VPC_ID } --output text --query GroupId ) REDIS_PORT = 6379 aws ec2 authorize-security-group-ingress --group-id ${ ELASTICACHE_SECURITY_GROUP_ID } --source-group ${ EKS_SECURITY_GROUP_ID } --protocol tcp --port ${ REDIS_PORT } Now you can define the ReplicationGroup using the following ytt template replication-group.ytt.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #@ load(\"@ytt:data\", \"data\") --- apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup metadata : name : ack-elasticache namespace : service-instances spec : description : A redis service instance engine : redis replicationGroupID : ack-elasticache cacheNodeType : cache.t2.micro cacheSubnetGroupName : #@ data.values.cacheSubnetGroupName securityGroupIDs : - #@ data.values.securityGroupID userGroupIDs : - ack-elasticache and apply it ytt \\ -v cacheSubnetGroupName = \" ${ CACHE_SUBNET_GROUP_NAME } \" \\ -v securityGroupID = \" ${ ELASTICACHE_SECURITY_GROUP_ID } \" \\ -f replication-group.ytt.yaml \\ | kubectl apply -f - It will take 5 to 10 minutes to create. You can wait for the resource to be ready running the command kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache or you can take a closer look at the new resource kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws ack-elasticache -o yaml particularly at the status field, which eventually will display something like status : ... conditions : - status : \"True\" type : ACK.ResourceSynced ... status : available The status object also contains the details of the provisioned AWS resource, along with the nodegroups and their endpoints.","title":"Create the ReplicationGroup"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-binding-specification-compatible-secret","text":"As mentioned in Creating service instances that are compatible with Tanzu Application Platform , in order for Tanzu Application Platform workloads to be able to claim and bind to services such as AWS Elasticache, a resource compatible with Service Binding Specification must exist in the cluster. This can take the form of either a ProvisionedService , as defined by the specification, or a Kubernetes Secret with some known keys, also as defined in the specification. In this guide, you create a Kubernetes secret in the necessary format using the secretgen-controller tooling. You do so by using the SecretTemplate API to extract values from the ACK resources and populate a new spec-compatible secret with the values.","title":"Create a Binding Specification Compatible Secret"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-serviceaccount-for-secret-templating","text":"As part of using the SecretTemplate API, a Kubernetes ServiceAccount must be provided. The ServiceAccount is used for reading the ReplicationGroup resource and the Secret created from the Password resource above. Create the following Kubernetes resources on your EKS cluster: rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 --- apiVersion : v1 kind : ServiceAccount metadata : name : ack-elasticache-reader namespace : service-instances --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : ack-elasticache-reader namespace : service-instances rules : - apiGroups : - \"\" resources : - secrets verbs : - get - list - watch resourceNames : - ack-elasticache-default-creds - apiGroups : - elasticache.services.k8s.aws resources : - replicationgroups verbs : - get - list - watch resourceNames : - ack-elasticache --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-elasticache-reader namespace : service-instances roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : ack-elasticache-reader subjects : - kind : ServiceAccount name : ack-elasticache-reader namespace : service-instances kubectl apply -f rbac.yaml","title":"Create a ServiceAccount for Secret Templating"},{"location":"usecases/aws/packages/elasticache/ack/manual/#create-a-secrettemplate","text":"In combination with the ServiceAccount just created, a SecretTemplate can be used to declaratively create a secret that is compatible with the service binding specification. For more information on this API see the Secret Template Documentation . Create the following Kubernetes resource on your EKS cluster: secrettemplate.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion : secretgen.carvel.dev/v1alpha1 kind : SecretTemplate metadata : name : ack-elasticache-default-creds-bindable namespace : service-instances spec : serviceAccountName : ack-elasticache-reader inputResources : - name : replicationGroup ref : apiVersion : elasticache.services.k8s.aws/v1alpha1 kind : ReplicationGroup name : ack-elasticache - name : creds ref : apiVersion : v1 kind : Secret name : ack-elasticache-default-creds template : metadata : labels : services.apps.tanzu.vmware.com/class : aws-elasticache type : servicebinding.io/redis stringData : type : redis username : $(.creds.data.username) ssl : \"true\" host : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.address) port : $(.replicationGroup.status.nodeGroups[0].primaryEndpoint.port) data : password : $(.creds.data.password) kubectl apply -f secrettemplate.yaml","title":"Create a SecretTemplate"},{"location":"usecases/aws/packages/elasticache/ack/manual/#verify-the-service-instance","text":"Wait until the ReplicationGroup instance is ready as described before . Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True secrettemplates.secretgen.carvel.dev ack-elasticache-default-creds-bindable kubectl -n service-instances get secret ack-elasticache-default-creds-bindable","title":"Verify the Service Instance"},{"location":"usecases/aws/packages/elasticache/ack/package/","text":"This topic describes creating, updating, and deleting AWS Elasticache instances using a Carvel package. For a more detailed and low-level alternative procedure, see Creating Service Instances that are compatible with Tanzu Application Platform . Add a reference package repository to the cluster \u00b6 The namespace tanzu-package-repo-global has a special significance. The kapp-controller defines a Global Packaging namespace. In this namespace, any package that is made available through a Package Repository is available in every namespace. When the kapp-controller is installed via Tanzu Application Platform, the namespace is tanzu-package-repo-global . If you install the controller in another way, verify which namespace is considered the Global Packaging namespace. You can use the following command to get the global namespace: GLOBAL_NAMESPACE = $( kubectl -n kapp-controller get deployment kapp-controller -o json | jq -r '.spec.template.spec.containers[]|select(.name==\"kapp-controller\").args[]|select(.|startswith(\"-packaging-global-namespace\"))|split(\"=\")[1]' ) To add a reference package repository to the cluster: Use the Tanzu CLI to add the new Service Reference packages repository: tanzu package repository add tap-reference-service-packages \\ --url ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages:0.0.3 \\ -n ${ GLOBAL_NAMESPACE } Create a ServiceAccount to provision PackageInstall resources by using the following example. The namespace of this ServiceAccount must match the namespace of the tanzu package install command in the next step. rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : v1 kind : ServiceAccount metadata : name : elasticache-install --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install rules : - apiGroups : [ \"elasticache.services.k8s.aws\" ] resources : [ \"*\" ] verbs : [ \"*\" ] - apiGroups : [ \"secretgen.carvel.dev\" , \"secretgen.k14s.io\" ] resources : [ \"secrettemplates\" , \"passwords\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" , \"configmaps\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : [ \"roles\" , \"rolebindings\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install subjects : - kind : ServiceAccount name : elasticache-install roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : elasticache-install kubectl apply -f rbac.yaml Create an AWS Elasticache instance through the Tanzu CLI \u00b6 In order to configure the package installation, you must provide a values file. Here are some values highlighted: namespace is the namespace where to deploy the AWS resources to, it may differ from the one dedicated to the package(s). The following example uses the service-instances namespace, do make sure it exists or set createNamespace: true . cacheSubnetGroupName is the name of the AWS CacheSubnetGroup to use for deploying the Elasticache instance. If it doesn't exist it can be created as part of the package setting the createCacheSubnetGroup flag to true and providing the subnetIDs list. vpcSecurityGroupIDs is a mandatory list of security group IDs that will be associated to the Elasticache instances and will filter network traffic to/from them. Warning Because of the ephemeral nature of such IDs, if the security groups are destroyed and re-created, the package will need to be updated with the new values, otherwise Elasticache instances will become unreachable. It is recommended to set the value of the name field below from redis to something unique, using only lowercase letters, digits and hyphens. Do make sure you also change the commands below using a redis value, such as the redis-writer-creds-bindable from the SecretTemplate, and replace redis with the actual name . Create a values file holding the configuration of the AWS Elasticache service instance: redis-instance-values.yml 1 2 3 4 5 6 7 8 9 10 --- name : redis namespace : service-instances cacheSubnetGroupName : redis-subnets replicasPerNodeGroup : 1 vpcSecurityGroupIDs : - sg-0a4ddae4fbf426cc8 tags : - key : Generator value : Carvel package Tip To understand which settings are available for this package you can run: tanzu package available get \\ --values-schema elasticache.aws.references.services.apps.tanzu.vmware.com/0.0.1-alpha This shows a list of all configuration options you can use in the redis-instance-values.yml file. Use the Tanzu CLI to install an instance of the reference service instance package. tanzu package install redis-instance \\ --package-name elasticache.aws.references.services.apps.tanzu.vmware.com \\ --version 0 .0.1-alpha \\ --service-account-name elasticache-install \\ --values-file redis-instance-values.yml \\ --wait You can install the elasticache.aws.references.services.apps.tanzu.vmware.com package multiple times to produce various AWS Elasticache instances. You create a separate <INSTANCE-NAME>-values.yml for each instance, set a different name value, and then install the package with the instance-specific data values file. Verify the AWS Resources \u00b6 Verify the creation status for the AWS Elasticache instance by inspecting the conditions in the Kubernetes API. To do so, run: kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws redis -o yaml After a few minutes, even up to 10 or more depending on how many replicas have been requested, you will be able to find the binding-compliant secrets produced by PackageInstall . Currently the package creates a reader and a writer user, each one with its own bindable secret. To view them, run: kubectl -n service-instances get secrettemplate redis-reader-creds-bindable -o jsonpath = \"{.status.secret.name}\" kubectl -n service-instances get secrettemplate redis-writer-creds-bindable -o jsonpath = \"{.status.secret.name}\" Verify the Service Instance \u00b6 First, wait until the Elasticache instance is ready. kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True --timeout = 5m secrettemplate redis-reader-creds-bindable kubectl -n service-instances get secret -n default redis-reader-creds-bindable The same applies to the redis-writer-creds-bindable resources. Summary \u00b6 You have learnt to use Carvel's Package and PackageInstall APIs to create an AWS Elasticache instance. If you want to learn more about the pieces that comprise this service instance package, see Creating AWS Elasticache Instances manually using kubectl . Now that you have this available in the cluster, you can learn how to make use of it by continuing where you left off in Consuming AWS Elasticache with ACK .","title":"Creating AWS Elasticache instances by using a Carvel package (experimental)"},{"location":"usecases/aws/packages/elasticache/ack/package/#add-a-reference-package-repository-to-the-cluster","text":"The namespace tanzu-package-repo-global has a special significance. The kapp-controller defines a Global Packaging namespace. In this namespace, any package that is made available through a Package Repository is available in every namespace. When the kapp-controller is installed via Tanzu Application Platform, the namespace is tanzu-package-repo-global . If you install the controller in another way, verify which namespace is considered the Global Packaging namespace. You can use the following command to get the global namespace: GLOBAL_NAMESPACE = $( kubectl -n kapp-controller get deployment kapp-controller -o json | jq -r '.spec.template.spec.containers[]|select(.name==\"kapp-controller\").args[]|select(.|startswith(\"-packaging-global-namespace\"))|split(\"=\")[1]' ) To add a reference package repository to the cluster: Use the Tanzu CLI to add the new Service Reference packages repository: tanzu package repository add tap-reference-service-packages \\ --url ghcr.io/vmware-tanzu/tanzu-application-platform-reference-service-packages:0.0.3 \\ -n ${ GLOBAL_NAMESPACE } Create a ServiceAccount to provision PackageInstall resources by using the following example. The namespace of this ServiceAccount must match the namespace of the tanzu package install command in the next step. rbac.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 --- apiVersion : v1 kind : ServiceAccount metadata : name : elasticache-install --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install rules : - apiGroups : [ \"elasticache.services.k8s.aws\" ] resources : [ \"*\" ] verbs : [ \"*\" ] - apiGroups : [ \"secretgen.carvel.dev\" , \"secretgen.k14s.io\" ] resources : [ \"secrettemplates\" , \"passwords\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"serviceaccounts\" , \"configmaps\" ] verbs : [ \"*\" ] - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"rbac.authorization.k8s.io\" ] resources : [ \"roles\" , \"rolebindings\" ] verbs : [ \"*\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : elasticache-install subjects : - kind : ServiceAccount name : elasticache-install roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : elasticache-install kubectl apply -f rbac.yaml","title":"Add a reference package repository to the cluster"},{"location":"usecases/aws/packages/elasticache/ack/package/#create-an-aws-elasticache-instance-through-the-tanzu-cli","text":"In order to configure the package installation, you must provide a values file. Here are some values highlighted: namespace is the namespace where to deploy the AWS resources to, it may differ from the one dedicated to the package(s). The following example uses the service-instances namespace, do make sure it exists or set createNamespace: true . cacheSubnetGroupName is the name of the AWS CacheSubnetGroup to use for deploying the Elasticache instance. If it doesn't exist it can be created as part of the package setting the createCacheSubnetGroup flag to true and providing the subnetIDs list. vpcSecurityGroupIDs is a mandatory list of security group IDs that will be associated to the Elasticache instances and will filter network traffic to/from them. Warning Because of the ephemeral nature of such IDs, if the security groups are destroyed and re-created, the package will need to be updated with the new values, otherwise Elasticache instances will become unreachable. It is recommended to set the value of the name field below from redis to something unique, using only lowercase letters, digits and hyphens. Do make sure you also change the commands below using a redis value, such as the redis-writer-creds-bindable from the SecretTemplate, and replace redis with the actual name . Create a values file holding the configuration of the AWS Elasticache service instance: redis-instance-values.yml 1 2 3 4 5 6 7 8 9 10 --- name : redis namespace : service-instances cacheSubnetGroupName : redis-subnets replicasPerNodeGroup : 1 vpcSecurityGroupIDs : - sg-0a4ddae4fbf426cc8 tags : - key : Generator value : Carvel package Tip To understand which settings are available for this package you can run: tanzu package available get \\ --values-schema elasticache.aws.references.services.apps.tanzu.vmware.com/0.0.1-alpha This shows a list of all configuration options you can use in the redis-instance-values.yml file. Use the Tanzu CLI to install an instance of the reference service instance package. tanzu package install redis-instance \\ --package-name elasticache.aws.references.services.apps.tanzu.vmware.com \\ --version 0 .0.1-alpha \\ --service-account-name elasticache-install \\ --values-file redis-instance-values.yml \\ --wait You can install the elasticache.aws.references.services.apps.tanzu.vmware.com package multiple times to produce various AWS Elasticache instances. You create a separate <INSTANCE-NAME>-values.yml for each instance, set a different name value, and then install the package with the instance-specific data values file.","title":"Create an AWS Elasticache instance through the Tanzu CLI"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-aws-resources","text":"Verify the creation status for the AWS Elasticache instance by inspecting the conditions in the Kubernetes API. To do so, run: kubectl -n service-instances get replicationgroups.elasticache.services.k8s.aws redis -o yaml After a few minutes, even up to 10 or more depending on how many replicas have been requested, you will be able to find the binding-compliant secrets produced by PackageInstall . Currently the package creates a reader and a writer user, each one with its own bindable secret. To view them, run: kubectl -n service-instances get secrettemplate redis-reader-creds-bindable -o jsonpath = \"{.status.secret.name}\" kubectl -n service-instances get secrettemplate redis-writer-creds-bindable -o jsonpath = \"{.status.secret.name}\"","title":"Verify the AWS Resources"},{"location":"usecases/aws/packages/elasticache/ack/package/#verify-the-service-instance","text":"First, wait until the Elasticache instance is ready. kubectl -n service-instances wait --for = condition = ACK.ResourceSynced = True replicationgroups.elasticache.services.k8s.aws ack-elasticache Next, ensure a bindable Secret was produced by the SecretTemplate . To do so, run: kubectl -n service-instances wait --for = condition = ReconcileSucceeded = True --timeout = 5m secrettemplate redis-reader-creds-bindable kubectl -n service-instances get secret -n default redis-reader-creds-bindable The same applies to the redis-writer-creds-bindable resources.","title":"Verify the Service Instance"},{"location":"usecases/aws/packages/elasticache/ack/package/#summary","text":"You have learnt to use Carvel's Package and PackageInstall APIs to create an AWS Elasticache instance. If you want to learn more about the pieces that comprise this service instance package, see Creating AWS Elasticache Instances manually using kubectl . Now that you have this available in the cluster, you can learn how to make use of it by continuing where you left off in Consuming AWS Elasticache with ACK .","title":"Summary"},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/","text":"Delete default user before group is deleted \u00b6 If a user named default is issued a delete command before the usergroup it belongs to has actually been deleted, ACK returns an unrecoverable error, thus preventing the deletion process to complete successfully. - message : \"DefaultUserAssociatedToUserGroup: User is associated to user group(s) as a default user and can't be deleted.\\n\\tstatus code: 400, request id: 65cb3646-e20f-4ba6-96c2-bf38ce13f78e\" status : \"True\" type : ACK.Terminal","title":"Troubleshooting AWS ACK"},{"location":"usecases/aws/packages/elasticache/ack/troubleshooting/#delete-default-user-before-group-is-deleted","text":"If a user named default is issued a delete command before the usergroup it belongs to has actually been deleted, ACK returns an unrecoverable error, thus preventing the deletion process to complete successfully. - message : \"DefaultUserAssociatedToUserGroup: User is associated to user group(s) as a default user and can't be deleted.\\n\\tstatus code: 400, request id: 65cb3646-e20f-4ba6-96c2-bf38ce13f78e\" status : \"True\" type : ACK.Terminal","title":"Delete default user before group is deleted"},{"location":"usecases/aws/prerequisites/","text":"This is a list of prerequisites that must be satisfied in order to deploy a TAP Reference Package on AWS EKS and consume it through TAP : Install ytt , a templating tool for YAML that is being widely used in these guides. Install the AWS CLI. For how to do so, see the AWS documentation . Log into AWS with your own credentials and assume a role with proper permissions to deal with EKS and Elasticache services. Check your AWS account number, user and assumed role running: aws sts get-caller-identity Make sure you have an available EKS cluster and the related OIDC provider configured. In order to create a new one you can follow this guide . Install Tanzu Application Platform v1.2.0 or later and Cluster Essentials v1.2.0 or later on the Kubernetes cluster. For more information, see Installing Tanzu Application Platform . Verify that you have the appropriate versions by running: kubectl api-resources | grep secrettemplate This command returns the SecretTemplate API. If it does not work for you, you might not have Cluster Essentials for VMware Tanzu v1.2.0 or later installed. Only if you want to deploy reference packages based on ACK : install the AWS Controller for Kubernetes (ACK) for the service(s) you are going to consume on AWS. Only if you want to deploy reference packages based on Crossplane : install Crossplane and the AWS provider.","title":"AWS prerequisites"},{"location":"usecases/aws/prerequisites/ack/","text":"AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes, by mapping AWS services to Kubernetes CRDs. There's a number of different controllers to manage different AWS services, with different levels of maturity, listed in the documentation . The following example shows how to install the Elasticache controller, but the same concept applies to all of them. Install ElastiCache Controller \u00b6 SERVICE = \"elasticache\" RELEASE_VERSION = ` curl -sL https://api.github.com/repos/aws-controllers-k8s/ $SERVICE -controller/releases/latest | grep '\"tag_name\":' | cut -d '\"' -f4 ` ACK_SYSTEM_NAMESPACE = \"ack-system\" AWS_REGION = \"eu-central-1\" aws ecr-public get-login-password --region us-east-1 | \\ helm registry login --username AWS --password-stdin public.ecr.aws helm install \\ ack- $SERVICE -controller \\ oci://public.ecr.aws/aws-controllers-k8s/ $SERVICE -chart \\ --create-namespace \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --version = $RELEASE_VERSION \\ --set = aws.region = $AWS_REGION Warning The --region flag in the aws ecr-public get-login-password command must be set either to us-east-1 or us-west-2 , as described in ECR public AWS documentation . Set the AWS_REGION variable according to your needs and configure the IAM role for ACK's service account . If you followed the guide for creating the EKS cluster v1.23+ you should have already configured the OIDC provider for authentication, therefore you can skip to configuring the IAM role and policy for the service account .","title":"Install the AWS Controllers for Kubernetes (ACK)"},{"location":"usecases/aws/prerequisites/ack/#install-elasticache-controller","text":"SERVICE = \"elasticache\" RELEASE_VERSION = ` curl -sL https://api.github.com/repos/aws-controllers-k8s/ $SERVICE -controller/releases/latest | grep '\"tag_name\":' | cut -d '\"' -f4 ` ACK_SYSTEM_NAMESPACE = \"ack-system\" AWS_REGION = \"eu-central-1\" aws ecr-public get-login-password --region us-east-1 | \\ helm registry login --username AWS --password-stdin public.ecr.aws helm install \\ ack- $SERVICE -controller \\ oci://public.ecr.aws/aws-controllers-k8s/ $SERVICE -chart \\ --create-namespace \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --version = $RELEASE_VERSION \\ --set = aws.region = $AWS_REGION Warning The --region flag in the aws ecr-public get-login-password command must be set either to us-east-1 or us-west-2 , as described in ECR public AWS documentation . Set the AWS_REGION variable according to your needs and configure the IAM role for ACK's service account . If you followed the guide for creating the EKS cluster v1.23+ you should have already configured the OIDC provider for authentication, therefore you can skip to configuring the IAM role and policy for the service account .","title":"Install ElastiCache Controller"},{"location":"usecases/aws/prerequisites/eks/","text":"An EKS cluster can be created in a number of ways, including AWS console, CLI, Terraform or CloudFormation. The quickest and simplest way to create an EKS cluster is to use eksctl , a CloudFormation wrapper, that will be used in this guide. Define the environment parameters \u00b6 The following are the parameters needed for the commands in this guide, which must be set the values that match your environment and needs. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # autoscaling group minimum number of nodes ASG_MIN_NODES = \"2\" # autoscaling group maximum number of nodes ASG_MAX_NODES = \"4\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # EKS kubernetes version to deploy KUBERNETES_VERSION = \"1.23\" Info The AWS_REGION variable must be an environment variable (thus the export ) to be used by eksctl and aws commands. The other variables can be just shell variables. Tip A list of available kubernetes versions for the KUBERNETES_VERSION variable can be obtained running aws eks describe-addon-versions --query \"addons[].addonVersions[].compatibilities[].clusterVersion\" | jq 'unique|sort' Create the EKS cluster \u00b6 The following command can create an EKS cluster based on the parameters defined above: eksctl create cluster -m ${ ASG_MIN_NODES } -M ${ ASG_MAX_NODES } -n ${ CLUSTER_NAME } --version ${ KUBERNETES_VERSION } The previous command waits until the cluster is created and also updates the KUBECONFIG file with the details of the new cluster. Configure the EBS CSI controller \u00b6 From version 1.23 onwards it is necessary to create the addon for the EBS CSI driver. aws eks create-addon --cluster-name ${ CLUSTER_NAME } --addon-name aws-ebs-csi-driver EKS pods' service accounts can assume AWS IAM roles to be able to authenticate and interact with the AWS APIs. They are mapped to web identities via an IAM OIDC provider that must be created for the EKS cluster. You will then need to create a proper role for the EBS CSI controller to be able to create and manage EBS volumes. # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) EBS_ROLE = AmazonEKS_EBS_CSI_Driver- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\" } } }] } EOF # create the role for the EBS CSI controller with the proper trust policy aws iam create-role --role-name ${ EBS_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # fetch a sample permission policy document curl -sSfL -o ${ ROLE_PERMISSION_POLICY } https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ EBS_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ EBS_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Next, you must prepare the OIDC provider (see also AWS documentation ). If the EKS cluster has just been created the OIDC provider does not exist yet, otherwise you can run the following command to make sure aws iam list-open-id-connect-providers | grep $OIDC_ID If no output is returned, the OIDC provider does not exist and you must create it. Option 1 (quick) Option 2 (know what you are doing) The eksctl does the heavy lifting for you and creates the OIDC provider with the proper configuration for your EKS cluster. eksctl utils associate-iam-oidc-provider --cluster ${ CLUSTER_NAME } --approve Get rid of the eksctl magic and configure the OIDC provider yourself. Get the SHA1 fingerprint for validating the OIDC provider certificate: OIDC_HOST = $( echo $OIDC_ID | cut -d/ -f1 ) SHA1_FINGERPRINT = $( { echo | openssl s_client -connect ${ OIDC_HOST } :443 -servername ${ OIDC_HOST } -showcerts | openssl x509 -fingerprint -noout -sha1 } 2 >/dev/null | cut -d = -f2 | sed s/://g ) Create the OIDC provider: aws iam create-open-id-connect-provider --url ${ OIDC_URL } --thumbprint-list ${ SHA1_FINGERPRINT } --client-id-list sts.amazonaws.com You must then configure the Kubernetes service account to assume the role # annotate the controller service account with the new role ARN kubectl -n kube-system annotate serviceaccount ebs-csi-controller-sa eks.amazonaws.com/role-arn = arn:aws:iam:: ${ ACCOUNT_ID } :role/ ${ EBS_ROLE } # restart ebs-csi-controller pods kubectl -n kube-system rollout restart deployment ebs-csi-controller Your EKS cluster is now configured to dynamically allocate EBS volumes. You need to create the proper StorageClass resources for your PersistentVolumeClaims to use, like in this example .","title":"Create an EKS cluster with EBS CSI driver"},{"location":"usecases/aws/prerequisites/eks/#define-the-environment-parameters","text":"The following are the parameters needed for the commands in this guide, which must be set the values that match your environment and needs. # AWS region you're operating in export AWS_REGION = \"eu-central-1\" # autoscaling group minimum number of nodes ASG_MIN_NODES = \"2\" # autoscaling group maximum number of nodes ASG_MAX_NODES = \"4\" # EKS cluster name CLUSTER_NAME = \"my-eks-cluster\" # EKS kubernetes version to deploy KUBERNETES_VERSION = \"1.23\" Info The AWS_REGION variable must be an environment variable (thus the export ) to be used by eksctl and aws commands. The other variables can be just shell variables. Tip A list of available kubernetes versions for the KUBERNETES_VERSION variable can be obtained running aws eks describe-addon-versions --query \"addons[].addonVersions[].compatibilities[].clusterVersion\" | jq 'unique|sort'","title":"Define the environment parameters"},{"location":"usecases/aws/prerequisites/eks/#create-the-eks-cluster","text":"The following command can create an EKS cluster based on the parameters defined above: eksctl create cluster -m ${ ASG_MIN_NODES } -M ${ ASG_MAX_NODES } -n ${ CLUSTER_NAME } --version ${ KUBERNETES_VERSION } The previous command waits until the cluster is created and also updates the KUBECONFIG file with the details of the new cluster.","title":"Create the EKS cluster"},{"location":"usecases/aws/prerequisites/eks/#configure-the-ebs-csi-controller","text":"From version 1.23 onwards it is necessary to create the addon for the EBS CSI driver. aws eks create-addon --cluster-name ${ CLUSTER_NAME } --addon-name aws-ebs-csi-driver EKS pods' service accounts can assume AWS IAM roles to be able to authenticate and interact with the AWS APIs. They are mapped to web identities via an IAM OIDC provider that must be created for the EKS cluster. You will then need to create a proper role for the EBS CSI controller to be able to create and manage EBS volumes. # define variables for IAM ACCOUNT_ID = $( aws sts get-caller-identity --query Account --output text ) OIDC_ID = $( aws eks describe-cluster --name ${ CLUSTER_NAME } --output text --query \"cluster.identity.oidc.issuer\" | cut -d/ -f3- ) EBS_ROLE = AmazonEKS_EBS_CSI_Driver- ${ CLUSTER_NAME } ROLE_TRUST_POLICY = $( mktemp ) ROLE_PERMISSION_POLICY = $( mktemp ) # prepare the trust policy document cat > ${ ROLE_TRUST_POLICY } <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ID}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\" } } }] } EOF # create the role for the EBS CSI controller with the proper trust policy aws iam create-role --role-name ${ EBS_ROLE } --assume-role-policy-document file:// ${ ROLE_TRUST_POLICY } # fetch a sample permission policy document curl -sSfL -o ${ ROLE_PERMISSION_POLICY } https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # create the permission policy PERMISSION_POLICY_ARN = $( aws iam create-policy --policy-name ${ EBS_ROLE } --policy-document file:// ${ ROLE_PERMISSION_POLICY } --query Policy.Arn --output text ) # attach the policy to the role aws iam attach-role-policy --policy-arn ${ PERMISSION_POLICY_ARN } --role-name ${ EBS_ROLE } # clean up temporary files rm ${ ROLE_TRUST_POLICY } rm ${ ROLE_PERMISSION_POLICY } Next, you must prepare the OIDC provider (see also AWS documentation ). If the EKS cluster has just been created the OIDC provider does not exist yet, otherwise you can run the following command to make sure aws iam list-open-id-connect-providers | grep $OIDC_ID If no output is returned, the OIDC provider does not exist and you must create it. Option 1 (quick) Option 2 (know what you are doing) The eksctl does the heavy lifting for you and creates the OIDC provider with the proper configuration for your EKS cluster. eksctl utils associate-iam-oidc-provider --cluster ${ CLUSTER_NAME } --approve Get rid of the eksctl magic and configure the OIDC provider yourself. Get the SHA1 fingerprint for validating the OIDC provider certificate: OIDC_HOST = $( echo $OIDC_ID | cut -d/ -f1 ) SHA1_FINGERPRINT = $( { echo | openssl s_client -connect ${ OIDC_HOST } :443 -servername ${ OIDC_HOST } -showcerts | openssl x509 -fingerprint -noout -sha1 } 2 >/dev/null | cut -d = -f2 | sed s/://g ) Create the OIDC provider: aws iam create-open-id-connect-provider --url ${ OIDC_URL } --thumbprint-list ${ SHA1_FINGERPRINT } --client-id-list sts.amazonaws.com You must then configure the Kubernetes service account to assume the role # annotate the controller service account with the new role ARN kubectl -n kube-system annotate serviceaccount ebs-csi-controller-sa eks.amazonaws.com/role-arn = arn:aws:iam:: ${ ACCOUNT_ID } :role/ ${ EBS_ROLE } # restart ebs-csi-controller pods kubectl -n kube-system rollout restart deployment ebs-csi-controller Your EKS cluster is now configured to dynamically allocate EBS volumes. You need to create the proper StorageClass resources for your PersistentVolumeClaims to use, like in this example .","title":"Configure the EBS CSI controller"}]}